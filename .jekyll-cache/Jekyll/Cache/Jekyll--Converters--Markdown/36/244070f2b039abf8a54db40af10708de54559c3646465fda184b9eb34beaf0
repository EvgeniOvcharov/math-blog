I"¥<p>Here we illustrate three of the famous zero-one laws for convergent sequences of random variables. Their names are: the <em>Borel-Cantelli lemma</em>, the <em>second Borel-Cantelli lemma</em>, and <em>Kolmogorovâ€™s zero-one law</em>. They are going to help us study the nuances in the relationship between almost sure convergence and complete convergence.</p>

<hr />

<p>Suppose we are given an infinite sequence of random variables</p>

<p>\begin{equation}\label{eq: X}
X_1, X_2, \dots
\end{equation}</p>

<p>defined on some probability space $(\Omega, \mathcal A, \operatorname P)$. We list three different modes in which the sequence may be convergent to zero.</p>

<ul>
  <li>$(i)$ The sequence $\eqref{eq: X}$ converges to zero in probability if for every $\epsilon&gt;0$</li>
</ul>

<script type="math/tex; mode=display">\begin{equation*}
  \lim_{n\rightarrow \infty} \operatorname P(|X_n|>\epsilon) = 0.
\end{equation*}</script>

<ul>
  <li>$(ii)$ The sequence $\eqref{eq: X}$ converges to zero almost surely if for every $\epsilon&gt;0$</li>
</ul>

<script type="math/tex; mode=display">\begin{equation*}
  \lim_{n\rightarrow \infty} \operatorname P(\{|X_n|> \epsilon\} \cup \{|X_{n+1}|> \epsilon\} \cup \cdots) = 0.
\end{equation*}</script>

<p>It is easily seen that this is equivalent to <script type="math/tex">{\displaystyle \operatorname P \left(\limsup _{n\to \infty }E_{n}\right)=0}</script>, which is equivalent to the usual condition $\operatorname P(\lim_{n\rightarrow\infty} X_n = 0)=1$.</p>

<ul>
  <li>$(iii)$ The sequence $\eqref{eq: X}$ converges to zero completely if for every $\epsilon&gt;0$</li>
</ul>

<script type="math/tex; mode=display">\begin{equation*}
  \lim_{n\rightarrow \infty} \operatorname P(\{|X_n|> \epsilon\}) + \operatorname P(\{|X_{n+1}|> \epsilon\}) + \cdots = 0.
\end{equation*}</script>

<p>Clearly, $(iii)$ implies $(ii)$ and $(i)$, and $(ii)$ implies $(i)$. The example: $\Omega = [0,1]$, $\operatorname P$ is the Lebesgue measure, $X_n = 1$ for $0&lt;\omega &lt; 1/n$, $X_n=0$ otherwise, shows that $(ii)$ does not imply $(iii)$.</p>

<p>We are going to show that $(ii)$ and $(iii)$ are equivalent if $X_n$ are independent. To that end, we need to revise some facts.</p>

<h3 id="borel-cantelli-lemma">Borel-Cantelli lemma</h3>

<hr />

<p>Let $E_1$, $E_2$,â€¦ be a sequence of events in some probability space. If the sum of the probabilities of $E_n$ is finite</p>

<script type="math/tex; mode=display">% <![CDATA[
{\displaystyle \sum _{n=1}^{\infty } \operatorname P(E_{n})<\infty ,} %]]></script>

<p>then the probability that infinitely many of them occur is $0$, that is,</p>

<script type="math/tex; mode=display">{\displaystyle \operatorname P \left(\limsup _{n\to \infty }E_{n}\right)=0.}</script>

<hr />

<p>The Borel-Cantelli lemma is an almost obvious fact which can be thought of as a more precise way of stating that complete convergence implies almost sure convergence, that is, that $(iii)$ implies $(ii)$. There is a partial converse to it, which is more useful to our goals.</p>

<h3 id="second-borel-cantelli-lemma">Second Borel-Cantelli lemma</h3>

<hr />

<p>If the events $E_n$ are independent and the sum of the probabilities of $E_n$ diverges to infinity,</p>

<script type="math/tex; mode=display">{\displaystyle \sum _{n=1}^{\infty }\operatorname P(E_{n})=\infty,}</script>

<p>then the probability that infinitely many of them occur is $1$,</p>

<script type="math/tex; mode=display">{\displaystyle \Pr(\limsup _{n\rightarrow \infty }E_{n})=1.}</script>

<hr />

<p>Denoting by $E_n$ the events  $E_n = \{ \mid X_n \mid &gt; \epsilon \}$, we have that they are independent under the assumption that $X_n$ are independent. If, in addition, $(ii)$ holds, but $(iii)$ does not, we reach to a contradiction in view of the second Borel-Cantelli lemma. Thus, under independence the latter two modes of convergences are equivalent.</p>

<p>The last thing we would like to touch upon here is the limit of a sequence of independent random variables. Such a limit may only be a constant, but in order to show this we need another fundamental convergence result.</p>

<h3 id="kolmogorovs-zeroone-law">Kolmogorovâ€™s zeroâ€“one law</h3>

<hr />
<p>Kolmogorovâ€™s zeroâ€“one law specifies that a certain type of event, called a <em>tail</em> event, will either almost surely happen or almost surely not happen; that is, the probability of such an event occurring is zero or one. Tail events are defined in terms of infinite sequences of random variables. Suppose $X_1, X_2, \dots$
is an infinite sequence of independent random variables (not necessarily identically distributed). Let ${\displaystyle {\mathcal {F}}}$ be the $\sigma$-algebra generated by all ${\displaystyle X_{i}}$ in the sequence. Then, a tail event ${\displaystyle F\in {\mathcal {F}}}$ is an event which is probabilistically independent of each finite subset of these random variables. Note that ${\displaystyle F}$ belonging to ${\displaystyle {\mathcal {F}}}$ implies that whether ${\displaystyle F}$ occurs or not is uniquely determined by the values of all ${\displaystyle X_{i}}$.</p>

<p>For example, the event that the sequence converges and the event that its sum converges are both tail events. In an infinite sequence of coin-tosses, a sequence of 100 consecutive heads occurring infinitely many times is a tail event. Intuitively, tail events are precisely those events whose occurrence can still be determined if an arbitrarily large but finite initial segment of the ${\displaystyle X_{i}}$ are removed. In many situations, it can be easy to apply Kolmogorovâ€™s zeroâ€“one law to show that some event has probability 0 or 1, but surprisingly hard to determine which of these two extreme values is the correct one.</p>

<h3 id="exact-formulation-of-kolmogorovs-zeroone-law">Exact formulation of Kolmogorovâ€™s zeroâ€“one law</h3>
<hr />
<p>Let $(\Omega, \mathcal A, \operatorname P)$ be a probability space and let $\mathcal F_n$ be a sequence of mutually independent $\sigma$-algebras contained in $\mathcal F_n$. Let</p>

<script type="math/tex; mode=display">{\displaystyle \mathcal G_{n}=\sigma {\bigg (}\bigcup _{k=n}^{\infty }\mathcal F_{k}{\bigg )}}</script>

<p>be the smallest $\sigma$-algebra containing $\mathcal F_n, \mathcal F_{n+1}, \dots$ Then for any event</p>

<script type="math/tex; mode=display">{\displaystyle F\in \bigcap _{n=1}^{\infty }\mathcal G_{n}}</script>

<p>one has either $\operatorname P(F) = 0$ or $1$.</p>

<hr />

<p>We now apply Kolmogorovâ€™s zeroâ€“one law to show that the limit $X$ of a sequence of independent random variables $X_n$ must be a constant. The argument is valid for multivariate random variables of arbitrary dimension $d$. We begin by noticing that any set of the form $\{ X \in A \mid A \in \mathcal B(\operatorname R^d)\}$ represents a tail event. Therefore, there must be a sufficiently large hypercub $Q_0$ with sides parallel to the coordinate axes and center at the origin such that $\operatorname P(X \in Q_0) = 1.$ Dividing each side by two, we partition $Q_0$ into $2^d$ smaller hypercubes. In view of Kolmogorovâ€™s zeroâ€“one law, there must be exactly one of these hypercubes, denoted by $Q_1$, such that $\operatorname P(X \in Q_1) = 1$. We continue this process iteratively and obtain an infinite sequence $Q_0, Q_1, \dots$ of nested hypercubes. Following  Cantorâ€™s argument, the intersection of all these hypercubes is a single point, $c$, and it follows that $\operatorname P(X = c) = 1$. This concludes the proof of the claim that $X$ must be a constant.</p>
:ET